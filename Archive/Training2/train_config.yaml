# MLX LoRA Fine-tuning Configuration

model:
  model_path: "lmstudio-community/Qwen3-30B-A3B-Thinking-2507-MLX-8bit"
  adapter_path: "adapters"

lora:
  fine_tune_type: "lora"  # Options: lora, dora, full
  num_layers: 8
  rank: 8
  alpha: 16  # Often set to 2*rank
  scale: 20.0
  dropout: 0.0
  # target_modules: []  # Optional: specific modules to apply LoRA to (defaults to all linear layers)

training:
  iterations: 100
  learning_rate: 2e-4
  batch_size: 1  # Adjust based on your GPU memory
  # gradient_accumulation_steps: 1  # Not supported in current MLX version
  steps_per_eval: 50
  steps_per_report: 10  # Steps between training loss reports
  val_batches: -1  # Number of validation batches, -1 for entire set
  steps_per_save: 100  # Steps between saving checkpoints
  grad_checkpoint: true  # Enable gradient checkpointing for memory efficiency
  seed: 42  # Random seed for reproducibility
  
  # Learning rate schedule (optional)
  lr_schedule:
    type: "cosine"  # Options: constant, linear, cosine
    warmup_steps: 100  # Number of warmup steps
    min_lr: 1e-6  # Minimum learning rate for cosine schedule
  
  # Gradient clipping (optional)
  gradient_clip:
    enabled: true
    max_norm: 1.0  # Maximum gradient norm

dataset:
  data_path: "../dataset_creation/final_dataset/final_cybersecurity_dataset_20250803_160211_hf"  # Path to HuggingFace dataset
  max_seq_length: 2048  # Maximum sequence length
  shuffle: true  # Whether to shuffle training data
  num_workers: 4  # Number of data loading workers

output:
  save_plots: true
  plot_path: "training_loss.png"
  save_metrics: true
  metrics_path: "training_metrics.json"
  checkpoint_dir: "checkpoints"  # Directory for saving checkpoints
  
  # Logging configuration (optional)
  logging:
    use_wandb: false  # Enable Weights & Biases logging
    wandb_project: "cybersecurity-lora"
    use_tensorboard: false  # Enable TensorBoard logging
    log_dir: "logs"

# Early stopping configuration (optional)
early_stopping:
  enabled: true
  patience: 5  # Number of evaluations with no improvement before stopping
  min_delta: 0.001  # Minimum change in validation loss to be considered improvement

# Test prompt for before/after comparison
test_prompt: "What is CAPEC-100?"

# Resume training configuration (optional)
resume:
  enabled: false
  checkpoint_path: null  # Path to checkpoint to resume from